---
layout: post
title: 正余弦位置编码
categories: [机器学习基础]
tags: 机器学习
---

## 0、背景知识

- 周期

1、正弦函数的一般形式为 $$y = A\sin(\omega x + \varphi)+b)$$，其周期\(T\)的计算公式为
$$T = \frac{2\pi}{|\omega|}$$

2、余弦函数的一般形式是$$y = A\cos(\omega x+\varphi)+b$$，它的周期计算公式和正弦函数一样，也是
$$T=\frac{2\pi}{|\omega|}$$。

## 一、为什么需要位置编码

之前NLP常用模型RNN、LSTM存在的问题

- 会出现“遗忘”的现象

- 句子越靠后的token对结果的影响越大；

- 只能利用上文信息，不能获取下文信息；

- 计算的时间复杂度比较高，循环网络是一个token一个token的输入的，也就是句子有多长就要循环多少遍；



Transformers没有位置编码会怎么样？

因为所有token一视同仁了，模型就没有办法知道每个token在句子中的相对和绝对的位置信息，而位置关系对于NLP任务来说是有着决定性影响的。比如下面的两句话token完全一样，但含义截然相反：

1.我把小姐姐按在地上摩擦

2.小姐姐把我按在地上摩擦

<img src="http://pointerhacker.github.io/imgs/posts/positional_encoding/image-20241224204254490.png" alt="image-20241224204254490" style="zoom:50%;" />

二、位置编码分类

### 1.绝对位置编码

 绝对位置编码为序列中的每个位置分配一个唯一的编码，这种编码直接反映了元素在序列中的绝对位置。

我  把  小  姐  姐  按  在  地  上  摩  擦

1   2   3  4   5   6  7   8   9  10  11

   但是这种方式存在很大的问题，句子越长，后面的值越大，**数字越大说明这个位置占的权重也越大，这样的方式无法凸显每个位置的真实的权重**，所以这种方式基本没有人用。

**基于正弦和余弦函数的编码**：由Transformer模型提出，使用正弦和余弦函数的不同频率来为序列中的每个位置生成唯一的编码。这种方法的优点是能够支持到任意长度的序列，并且模型可以从编码中推断出位置信息。本文将重点介绍这个。

**可学习的位置编码：**一些模型选择通过训练过程中学习位置编码，而不是使用预定义的函数。这种方法允许模型自适应地优化位置编码，以最适合特定任务的方式。但是这种方式一般要求固定输入长度，而且可解释性差，所以使用的也不多。不过可以举个例子，将绝对位置转换成位置编码向量，这种操作类似于embedding：



### **2.**相对位置编码

​     函数型位置编码通过输入token的位置信息，得到相应的位置编码。这种编码方式使模型能够更关注元素之间的相对位置关系。

​     虽然原始的Transformer模型使用的是绝对位置编码，但后续的研究和变体模型中已经引入了相对位置编码的概念，以期望提高模型对序列中元素之间相对位置关系的理解能力。例如：Transformer-XL和T5模型就采用了相对位置编码。篇幅原因（懒），相对位置编码我们以后再说。



## 三、Transformer的位置编码

### 1.位置编码应有的特点

好的位置编码应该有如下特点：

1. 每个时间步都有**唯一**的编码
2. 在不同长度的句子中，两个时间步之间的**距离应该一致**。
3. 模型不受句子长短的影响，并且**编码范围是有界**的。（不会随着句子加长数字就无限增大）
4. 同一时间序列中，每个连续的编码应该是**等步长**的，即线性相关的。
5. 位置编码应该能表示token的**绝对或相对位置关系**。

下面我们就来看一下Transformer的位置编码，是否符合上面几点要求。

### 2.Transformer的位置编码公式

使用正余弦函数表示绝对位置，通过两者乘积得到相对位置：

$$PE_{pos,2i} = sin(\frac {pos}{10000^ \frac{2^i}{d_{model}}})$$     

$$PE_{pos,2i+1} = cos(\frac {pos}{10000^{\frac{2^i}{d_{model}}}})$$   （1）

其中PE表示位置编码；pos 表示一句话中token的位置；每个token的位置编码是一个向量（或者叫embedding），i表示这个向量中每个元素的index；$$d_{model}$$表示位置编码向量的维度。

 式1，其实是一个式子，看起来非常复杂，写的直观一点是下面这个样子

$$\begin{bmatrix} sin(w_{1} \;\cdot \; pos ) \\ cos(w_{1} \;\cdot \; pos )\\  sin(w_{2} \;\cdot \; pos )\\ cos(w_{2} \;\cdot \; pos ) \\... \\sin(w_{d/2} \;\cdot \; pos ) \\ cos(w_{d/2} \;\cdot \; pos ) \end{bmatrix} $$ （2）

其中 $$w_{i} = \frac {1}{10000^ \frac{2^i}{d_{model}}}$$  

频率  $$T = 2\pi 10000^{\frac{2^i}{d_{model}}}$$.



### 3.编码因子

 **a.指数级频率变化**

通过 $$w_{i}$$ 这个因子，不同维度的波长呈指数级变化。

**指数函数允许位置编码在非常宽的频率范围内分布，从非常低的频率到非常高的频率。**这种广泛的覆盖确保了模型可以同时捕捉到长距离和短距离的依赖关系，适应不同的序列长度和结构

指数级的变化使得每个维度的波长快速变化，这意味着即使在较低的维度，也能有效地编码位置信息。

> 在dim维度，正余弦函数频率呈现指数级频率变化。允许位置编码在非常宽的频率范围内分布，从非常低的频率到非常高的频率，这种广泛的覆盖确保了模型可以同时捕捉到长距离和短距离的依赖关系，适应不同的序列长度和结构

  **b.平滑的频率变化**

​     因为模型要处理高维特征，$$d_{model}$$越大，$$10000^ \frac{2^i}{d_{model}}$$的增长越慢。这是因为指数$$\frac {2^i}{d_{model}}$$  变小，导致整个表达式的增长率下降。频率与$$\frac {pos}{10000^ \frac{2^i}{d_{model}}}$$的变化速度有关。当$$10000^ {\frac{2^i}{d_{model}}}$$增长慢时，相对于位置 pos 的变化，这个比值变化得更慢，导致频率变化缓慢。

​     当频率变化较慢时，相邻位置的编码差异较小，这使得模型可以更容易地捕捉到位置的连续性和顺序性。这种**平滑的变化有助于模型在学习过程中不会对位置的微小变化过度敏感，从而更好地泛化。**

​     在处理长序列时，平滑的频率变化意味着即使在序列的远端，位置编码仍然能够有效地区分不同的位置。这对于模型来说是非常重要的，因为它需要在整个序列中维持对位置的敏感性，以便正确地解释和预测序列中的信息。

> 由于正余弦函数在0-1之间变化的特性。不同位置相同dim维度的变化差异较小，这使得模型可以更容易地捕捉到位置的连续性和顺序性。这种**平滑的变化有助于模型在学习过程中不会对位置的微小变化过度敏感，从而更好地泛化。**

  **c.远程衰减性**

​     随着两个token相对**距离的增加，它们的相关性越来越弱**，呈现出远程衰减性，这个有相关推导，这里就不赘述了，知道结论就可以了。

​    **d.10000** 

是一个经验选择的常数，用于调节频率的下降速度，这个**数越大下降速度就越慢**。它被选为一个足够大的数，以确保即使在**模型维度很大时，频率的变化也能覆盖一个广泛的范围**。



### 4.三角函数

 从式（2）中可以发现Transformer的位置编码使用了三角函数，三角函数是周期函数，那为什么要把每个维度设计成周期形式呢，可以看下面这个图，二进制格式表示一个数字：

<img src="http://pointerhacker.github.io/imgs/posts/positional_encoding/42akycbre2ywi_e5db59f18789400c82b1fde4d2be3c40.png" alt="image.png" style="zoom:50%;" />

  可以看到以列为单位，不同位置上的数字都会出现0、1的交替变化。规律是第i位置上第i+1个数据交替一次。比如第0列（右面数第一列）是每1次都切换，第1列（右面数第二列）是每两次切换。但是在浮点数的世界中使用二进制值是对空间的浪费，所以我们可以用正弦函数代替。事实上，正弦函数也能表示出二进制那样的交替。随着波长的变长，带来的是数据变化的变慢，就如同上面的提到的。

Transformer的位置编码选择三角函数的官方解释是这样的：

位置编码的每个维度都对应于一个正弦曲线。波长形成一个从2π到10000·2π的几何轨迹。我们之所以选择这个函数，是因为我们假设它可以让模型很容易地通过相对位置进行学习，因为对于任何固定的偏移量k,$$PE_{pos+k}$$都可以表示为$$PE_{pos}$$的线性函数。

> 让我们来一步步分析这个数学表达式：
>
> 1. 首先，这是一个正弦函数的比值：sin(w(x+k)) / sin(wx)
> 2. 可以将分子展开：sin(w(x+k)) = sin(wx + wk)
> 3. 根据正弦的和角公式： sin(A + B) = sin(A)cos(B) + cos(A)sin(B)
> 4. 应用到分子： sin(wx + wk) = sin(wx)cos(wk) + cos(wx)sin(wk)
> 5. 因此，原式可以写为： [sin(wx)cos(wk) + cos(wx)sin(wk)] / sin(wx)
> 6. 约分掉分母和分子中的sin(wx)： = cos(wk) + cos(wx)sin(wk)/sin(wx)
>
> 最终结果为：cos(wk) + cot(wx)sin(wk)
>
> $$PE_{pos+k} = [cos(wk) + cot(wx)sin(wk)] \times PE_{pos} $$ 

​     也就是说，每个维度都是波长不同的正弦波，波长范围是2π到10000·2π，选用10000这个比较大的数是因为三角函数式有周期的，在这个范围基本上，就不会出现波长一样的情况了。然后谷歌的科学家们为了让PE值的周期更长，还交替使用sin/cos来计算PE的值，就得到了最终的公式（1）。

​     下图可以比较清晰的表现这过程。相同颜色的是同一个token的词向量，为了方便展示，波形图只使用sin，而且波形图只是展示使用，并不是实际波形图：

![image.png](http://pointerhacker.github.io/imgs/posts/positional_encoding/42akycbre2ywi_68b06186b3654c1c986f8202746ce063.png)

> ps: 第一列应该是i,pos 

可以看到，每个维度（即每一行）都是三角函数的波形，随着i的增加，波长越来越长，这就很好的实现了低位变化快、高位变化慢的情况。

​    在实践中Transformer通过在偶数和奇数维度上使用正弦和余弦函数，**位置编码向量的每个维度都能够保持一定的正交性。正交性确保了位置编码在不同维度上的独立表示，这有助于模型更清晰地区分和学习来自不同位置的特征。**例如，如果两个位置的编码在多个维度上都有显著差异，模型可以更容易地识别这两个位置是不同的。

> 两个函数的正交性是指这两个函数的内积为零

​     那每个pos内（即每一列）的元素是否为线性相关呢？

### 5、相对位置关系

### 

​     虽然这种方法是基于绝对位置的，但是也能一定程度的捕捉相对位置信息。这是因为自注意力机制能够利用这些位置编码之间的差异来推断元素之间的相对距离。

​     我们将$$PE_{pos}$$和$$PE_{pos+k}$$相乘 (两个向量的内积)，可以得到如下结果：

$$PE_{pos} \times PE_{pos+k} = \sum{^{d/2-1}} (cos_{w_{i}}.k) $$

  我们发现相乘后的结果为一个余弦的加和。这里影响值的因素就是k 。如果两个token的距离越大，也就是K越大，根据余弦函数的性质可以知道，两个位置的PE相乘结果越小。这样的关系可以得到，如果两个token距离越远则乘积的结果越小，位置编码可以表示相对位置关系。

​     尽管Transformer的原始位置编码能够间接地表示相对位置信息，但这种表示并不是直接或显式的。这意味着模型可能需要更多的训练数据和学习能力来充分利用相对位置信息，特别是在处理长序列和复杂依赖关系时
