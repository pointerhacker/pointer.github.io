---
layout: post
title: 深度强化学习门指南
categories: [RL]
tags: RL
---

## 什么是深度强化学习？

> Deep Reinforcement Learning

深度强化学习是一种机器学习，代理**通过执行操作**并**查看结果**来学习**如何在环境中表现**。

强化学习是一个解决控制任务（也称为决策问题【decision problems】）的框架，它通过构建**代理**来学习环境，通过反复试验与环境进行交互，并接收奖励（正面或负面）作为动作的反馈。

### The RL Process  强化学习过程

<img src="http://pointerhacker.github.io/imgs/posts/Introduction_rl/RL_process.jpg" alt="The RL process" style="zoom: 50%;" />

为了理解 RL 过程，让我们想象一个agent学习玩平台游戏：

![The RL process](http://pointerhacker.github.io/imgs/posts/Introduction_rl/RL_process_game.jpg)

- 我们的代理收到来自环境状态$S_{0}$ —— 我们收到游戏的第一帧（环境）。
- 基于该状态 $S_{0}$，代理采取行动$A_{0}$ —— 我们的特工将移至右侧。
- 环境进入新状态$S_{1}$— 新环境。
- 环境给予一些奖励$R_{1}$对特工来说——我们还没有死（正奖励+1） 。


该强化学习循环输出一系列**状态、动作、奖励和下一个状态**。

![State, Action, Reward, Next State](http://pointerhacker.github.io/imgs/posts/Introduction_rl/sars.jpg)

代理的目标是最大化其累积奖励，称为预期回报。

### 奖励假设：强化学习的中心思想

⇒ 为什么代理的目标是最大化预期回报？
因为RL是基于奖励假设的，即所有目标都可以描述为期望回报（期望累积奖励）的最大化。这就是为什么在强化学习中，为了获得最佳行为，我们的目标是学习采取能够最大化预期累积奖励的行动。

在论文中，您会看到 RL 过程称为马尔可夫决策过程(MDP)。我们将在下面的单元中再次讨论马尔可夫性质。但如果你今天需要记住一些关于它的事情，那就是：马尔可夫性质意味着我们的代理只需要**当前状态来决定采取什么行动**，而不是他们之前采取的所有状态和行动的历史。

### 观察/状态空间

观察/状态是**我们的代理从环境中获取的信息。**对于视频游戏，它可以是一个帧（屏幕截图）。
对于交易代理来说，它可以是某种股票的价值等。

然而，*观察*和*状态*之间存在区别：

- *State s* *状态*：是**对世界状态的完整描述**（没有隐藏信息）。在充分观察的环境中。
  - 王者荣耀里面的观战视角 【上帝视角】

- *Observation o *观察* ：是**状态的部分描述。**在部分观察的环境中。
  - 王者荣耀中的玩家视角 【NPC视角】


> 在本文中，我们使用术语“状态”来表示状态和观察，但我们将在实现中进行区分。



### 行动空间

动作空间是**环境中所有可能动作的集合。**

这些动作可以来自*离散*或*连续空间*：

- *离散空间*：可能的动作数量是**有限的**。
  - 象棋中的小兵 动作真可以是 前、左、右三种
- *连续空间*：可能的动作数量是**无限的**。
  - 英雄的移动是一个360度方向的全方位移动

考虑这些信息至关重要，因为它**在将来选择 RL 算法时非常重要。**



### 奖励和折扣

奖励在强化学习中至关重要，因为它是智能体的**唯一反馈**。多亏了它，我们的代理才知道**所采取的行动是否有效。**

每个时间步**t**的累积奖励可以写为：

![Rewards](http://pointerhacker.github.io/imgs/posts/Introduction_rl/rewards_1.jpg)

累积奖励等于序列中所有奖励的总和。这相当于：

<img src="http://pointerhacker.github.io/imgs/posts/Introduction_rl/rewards_2.jpg" alt="Rewards" style="zoom:50%;" />

然而，实际上，**我们不能就这样添加它们。**较早到来的奖励（在游戏开始时）**更有可能发生，**因为它们比长期的未来奖励更可预测。

假设您的代理是一只小老鼠，每次可以移动一个方块，而您的对手是猫（也可以移动）。老鼠的目标是在被猫吃掉之前吃掉最大数量的奶酪。

<img src="http://pointerhacker.github.io/imgs/posts/Introduction_rl/rewards_3.jpg" alt="Rewards" style="zoom:50%;" />

正如我们在图中看到的，**吃我们附近的奶酪的可能性比吃靠近猫的奶酪的可能性更大**（我们离猫越近，就越危险）。因此，**猫附近的奖励，即使它更大（更多奶酪），也会受到更大的折扣，**因为我们不确定我们是否能够吃掉它。再比如我们玩王者荣耀的时候，随着游戏时间的变长。如果大家都是6神装后获得的经济作用就越来越小【秒换复活甲/金身/名刀】

为了折扣奖励，我们这样做：

1. 我们定义一个称为gamma的贴现率。**它必须介于 0 和 1 之间。**大多数情况下介于**0.95 和 0.99**之间。
   - gamma 越大，折扣越小。这意味着我们的代理人**更关心长期回报。**
   - 另一方面，gamma越小，折扣就越大。这意味着我们的**代理更关心短期奖励（最近的奶酪）。**
2. 然后，每个奖励折扣将按 gamma 时间步的指数增长。随着时间步长的增加，猫离我们越来越近，**因此未来奖励发生的可能性越来越小**

我们的折扣预期累积奖励是：

![Rewards](http://pointerhacker.github.io/imgs/posts/Introduction_rl/rewards_4.jpg)



## 任务类型

任务是强化学习问题的一个**实例**。我们可以有两种类型的任务：**情景任务**和**持续任务**。

### 情景任务

在这种情况下，我们有一个起点和一个终点（**最终状态**）。**这将创建一个情节**：状态、操作、奖励和新状态的列表。例如，想想《超级马里奥兄弟》：场景从新的马里奥关卡启动时开始，**到你被杀或到达关卡末尾时结束。**

### 持续任务

这些是永远持续的任务（**没有最终状态**）。在这种情况下，智能体必须**学习如何选择最佳动作并同时与环境交互。**

例如，进行自动股票交易的代理。对于此任务，没有起点和终点状态。**代理会一直运行，直到我们决定停止它。**



## 探索/利用的权衡

最后，在研究解决强化学习问题的不同方法之前，我们必须讨论一个非常重要的主题：*探索/利用权衡。*

- *探索*是通过尝试随机动作来探索环境，以**找到有关环境的更多信息。**
- *利用*是**利用已知信息来最大化回报。**

请记住，我们的 RL 代理的目标是最大化预期累积奖励。然而，**我们可能会陷入一个常见的陷阱**。

 让我们举个例子：

![Exploration](http://pointerhacker.github.io/imgs/posts/Introduction_rl/exp_1.jpg)

在这个游戏中，我们的老鼠可以拥有**无限数量的小奶酪**（每个+1）。但在迷宫的顶部，有一大笔奶酪（+1000）。然而，如果我们只关注利用，我们的代理永远无法达到奶酪的巨额。相反，它只会利用**最近的奖励来源，**即使这个来源很小（利用）。但如果我们的智能体进行一点探索，它就能**发现巨大的奖励**（一堆大奶酪）。这就是我们所说的探索/利用权衡。我们需要平衡我们**对环境的探索**程度和我们**对环境已知信息的利用**。因此，我们必须**定义一个有助于处理这种权衡的规则**。我将在未来的单元中看到处理它的不同方法。如果还是很困惑，**那么想一个真正的问题：选择餐厅的选择：**

![Exploration](http://pointerhacker.github.io/imgs/posts/Introduction_rl/exp_2.jpg)

- *利用*：你每天都去同一家你知道很好的餐厅，却**冒着错过另一家更好的餐厅的风险。**
- *探索*：尝试您以前从未去过的餐厅，冒着获得糟糕体验的风险**，但也有可能获得美妙的体验。**

## 解决 RL 问题的两种主要方法

换句话说，我们如何构建一个 RL 代理来**选择最大化其预期累积奖励的动作？**

### 策略 π：代理的大脑

策略**π**是**Agent 的大脑**，它的功能告诉我们**在给定的状态下要采取什么行动。**因此它**定义了 Agent 在给定状态的行为**。

![Policy](http://pointerhacker.github.io/imgs/posts/Introduction_rl/policy_1.jpg)

> 将政策视为我们代理的大脑，该功能将告诉我们在给定状态下要采取的行动

这个Policy**就是我们想要学习的函数**，我们的目标是找到最优策略π*，即当代理按照它行动时使**期望回报最大化的**策略。我们**通过训练找到这个π\*。**

有两种方法可以训练我们的代理找到最优策略 π*：

- **直接地，**通过教导代理学习在给定当前状态的情况下**采取哪些行动**：**基于策略的方法。**
- 间接地，**教导代理了解哪种状态更有价值**，然后采取**导致更有价值状态**的行动：基于价值的方法。

###  基于策略的方法

在基于策略的方法中，**我们直接学习策略函数。**该函数将定义从每个状态到最佳对应动作的映射。或者，它可以定义**该状态下一组可能动作的概率分布**

![Policy](http://pointerhacker.github.io/imgs/posts/Introduction_rl/policy_2.jpg)

> 正如我们在这里看到的，策略（确定性）**直接指示每个步骤要采取的操作。**

我们有两种类型的政策：

- *确定性*：给定状态的策略**将始终返回相同的操作。**

  ![Pbm recap](http://pointerhacker.github.io/imgs/posts/Introduction_rl/pbm_1.jpg)

- *随机*：输出**动作的概率分布。**

  <img src="http://pointerhacker.github.io/imgs/posts/Introduction_rl/pbm_2.jpg" alt="Pbm recap" style="zoom:50%;" />



### 基于价值的方法

在基于价值的方法中，我们不是学习策略函数，而是学习将状态映射到**该状态的**期望值**的价值函数**。状态的价值是代理**在该状态开始然后根据我们的策略采取行动**时可以获得的**预期折扣回报**。

![Value based RL](http://pointerhacker.github.io/imgs/posts/Introduction_rl/value_1.jpg)

在这里我们看到我们的价值函数**为每个可能的状态定义了值。**

![Value based RL](http://pointerhacker.github.io/imgs/posts/Introduction_rl/value_2.jpg)

> 由于我们的价值函数，我们的策略在每一步都会选择价值函数定义的最大价值的状态：-7，然后-6，然后-5（依此类推）来实现目标。



## 强化学习的“深度”

深度强化学习引入**深度神经网络来解决强化学习问题**——因此得名“深度”。例如，在下一个单元中，我们将学习两种基于值的算法：Q-Learning（经典强化学习）和深度 Q-Learning。您会发现不同之处在于，在第一种方法中，**我们使用传统算法**创建 Q 表，帮助我们找到针对每个状态采取的操作。在第二种方法中，**我们将使用神经网络**（近似 Q 值）。

![Value based RL](http://pointerhacker.github.io/imgs/posts/Introduction_rl/deep.jpg)

## 概括

这是很多信息！我们总结一下：

- 强化学习是一种从**动作中学习**的计算方法。我们构建了一个代理，**通过反复试验与环境进行交互**，并接收奖励（负面或正面）作为反馈，从而从环境中学习。
- 任何 RL 代理的目标都是最大化其期望累积奖励（也称为期望回报），因为 RL 是基于**奖励假设的**，即**所有目标都可以描述为期望累积奖励的最大化。**
- 强化学习过程是一个循环，输出一系列**状态、动作、奖励和下一个状态。**
- 为了计算预期累积奖励（预期回报），我们对奖励进行折扣：较早出现的奖励（在游戏开始时）**更有可能发生，因为它们比长期未来奖励更可预测。**
- 要解决 RL 问题，您需要**找到最优策略**。该策略是代理的“大脑”，它将告诉我们**在给定的状态下采取什么操作。**最佳策略是为**您提供最大化预期回报的行动的策略。**
- 有两种方法可以找到最佳策略：
  1. 通过直接训练您的策略：**基于策略的方法。**
  2. 通过训练一个价值函数，该函数告诉我们代理在每个状态下将获得的预期回报，并使用该函数来定义我们的策略：**基于价值的方法**
- 最后，我们谈论深度强化学习，因为我们引入**深度神经网络来估计要采取的操作（基于策略）或估计状态的值（基于值），**因此称为“深度”。
