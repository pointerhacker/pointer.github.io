# RNN和LSTM

## RNN

### 什么是RNN

rnn即循环神经网络，模型当前的输入不仅仅有现在的隐藏状态决定还由之前的状态沟通决定

![img](http://pointerhacker.github.io/imgs/posts/rnn/v2-b0175ebd3419f9a11a3d0d8b00e28675_1440w.jpg)

我们现在这样来理解，如果把上面有W的那个带箭头的圈去掉，它就变成了最普通的**[全连接神经网络](https://zhida.zhihu.com/search?content_id=4526447&content_type=Article&match_order=1&q=全连接神经网络&zhida_source=entity)**。x是一个向量，它表示**输入层**的值 。s是一个向量，它表示**隐藏层**的值；

U是输入层到隐藏层的**权重矩阵**，o也是一个向量，它表示**输出层**的值；V是隐藏层到输出层的**[权重矩阵](https://zhida.zhihu.com/search?content_id=4526447&content_type=Article&match_order=2&q=权重矩阵&zhida_source=entity)**。



### 公式

![img](http://pointerhacker.github.io/imgs/posts/rnn/v2-9524a28210c98ed130644eb3c3002087_1440w.jpg)



### 代码

```python
from torch import nn
rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)
```

- `input_size` 表示输入特征的维度，也就是每个时间步输入数据的特征数量。
- `hidden_size` 是隐藏层状态的维度，决定了 RNN 内部状态表示的丰富程度。
- `num_layers` 是 RNN 的层数。



## LSTM

### 什么是LSTM

[长短期记忆](https://zhida.zhihu.com/search?content_id=5023272&content_type=Article&match_order=1&q=长短期记忆&zhida_source=entity)（Long short-term memory, LSTM）是一种特殊的RNN

![img](http://pointerhacker.github.io/imgs/posts/rnn/v2-e4f9851cad426dfe4ab1c76209546827_1440w.jpg)

相比RNN只有一个传递状态 $h^t$ ，LSTM有两个传输状态，一个 $c^t$（[cell state](https://zhida.zhihu.com/search?content_id=5023272&content_type=Article&match_order=1&q=cell+state&zhida_source=entity)），和一个 $h^t$ （[hidden state](https://zhida.zhihu.com/search?content_id=5023272&content_type=Article&match_order=1&q=hidden+state&zhida_source=entity)）。（Tips：RNN中的 $h^t$ 对于LSTM中的  $c^t$）

其中对于传递下去的  $c^t$ 改变得很慢，通常输出的  $c^t$是上一个状态传过来的  $c^{t-1}$加上一些数值。

而  $h^t$  则在不同节点下往往会有很大的区别。

###  [深入LSTM](https://zhida.zhihu.com/search?content_id=5023272&content_type=Article&match_order=1&q=深入LSTM&zhida_source=entity)结构

首先使用LSTM的当前输入 $x^t$ 和上一个状态传递下来的 $h^{t−1}$ 拼接训练得到四个状态。



![img](http://pointerhacker.github.io/imgs/posts/rnn/v2-15c5eb554f843ec492579c6d87e1497b_1440w.jpg)

![img](http://pointerhacker.github.io/imgs/posts/rnn/v2-d044fd0087e1df5d2a1089b441db9970_1440w.jpg)



其中， $z^f$ ， $z^i$ ，$z^o$ 是由拼接向量乘以[权重矩阵]之后，再通过一个 sigmoid 激活函数转换成0到1之间的数值，来作为一种门控状态。而 z 则是将结果通过一个 tanh [激活函数]将转换成-1到1之间的值（这里使用 tanh 是因为这里是将其做为输入数据，而不是[门控信号]）。

**下面开始进一步介绍这四个状态在LSTM内部的使用。（敲黑板）**

![img](http://pointerhacker.github.io/imgs/posts/rnn/v2-556c74f0e025a47fea05dc0f76ea775d_1440w.jpg)

$\bigodot$ 是Hadamard Product，也就是操作矩阵中对应的元素相乘，因此要求两个[相乘矩阵](https://zhida.zhihu.com/search?content_id=5023272&content_type=Article&match_order=1&q=相乘矩阵&zhida_source=entity)是同型的。 $\bigoplus$ 则代表进行[矩阵加法](https://zhida.zhihu.com/search?content_id=5023272&content_type=Article&match_order=1&q=矩阵加法&zhida_source=entity)。



LSTM内部主要有三个阶段：

1. 忘记阶段。这个阶段主要是对上一个节点传进来的输入进行**选择性**忘记。简单来说就是会 “忘记不重要的，记住重要的”。

   > 具体来说是通过计算得到的 $z^f$ （f表示forget）来作为忘记门控，来控制上一个状态的 $c^{t−1}$ 哪些需要留哪些需要忘。

2. 选择记忆阶段。这个阶段将这个阶段的输入有选择性地进行“记忆”。主要是会对输入 $x^t$ 进行选择记忆。哪些重要则着重记录下来，哪些不重要，则少记一些。当前的输入内容由前面计算得到的 z 表示。而选择的门控信号则是由 $z^i$ （i代表information）来进行控制。

> 将上面两步得到的结果相加，即可得到传输给下一个状态的 $c^t$ 。也就是上图中的第一个公式。

3. 输出阶段。这个阶段将决定哪些将会被当成当前状态的输出。主要是通过 $z^o$ 来进行控制的。并且还对上一阶段得到的 $c^o$ 进行了放缩（通过一个tanh激活函数进行变化）。

与普通RNN类似，输出 $y^t$ 往往最终也是通过 $h^t$ 变化得到。

### 代码

```python
lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
```

- `input_size` 代表输入特征的维度，即每个时间步输入数据的特征数量。
- `hidden_size` 是 LSTM 隐藏层状态的维度，它影响着网络对信息的记忆和处理能力。
- `num_layers` 表示 LSTM 的层数。

## 3. 总结

以上，就是LSTM的内部结构。通过门控状态来控制传输状态，记住需要长时间记忆的，忘记不重要的信息；而不像普通的RNN那样只能够“呆萌”地仅有一种记忆叠加方式。对很多需要“[长期记忆](https://zhida.zhihu.com/search?content_id=5023272&content_type=Article&match_order=1&q=长期记忆&zhida_source=entity)”的任务来说，尤其好用。
