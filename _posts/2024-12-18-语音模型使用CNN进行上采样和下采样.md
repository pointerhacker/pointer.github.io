
---
layout: mypost
title: 语音模型使用CNN进行上采样和下采样
categories: [多模态LLM]
extMath: true
---

# 语音模型使用CNN进行上采样和下采样

## 名称解释

- fram
  - 帧 是数据传输或处理中的基本单位。可以将它类比为一个窗口
  - 视频和动画中一帧代表一个静止的图像。
  - 音频中帧是一段固定时间长度的音频数据
- sample_rate
  - 采样率 单位时间采集点点数量
- fram_rate: 帧率 单位时间内采集窗口的数量 
- fram_size 基本窗口的时间点大小



## 背景介绍

为什么写这篇文章：最近在研究Moshi的代码过程中，发现论文中【while Mimi projects a 24kHz waveform into 512-dimensional at 12.5Hz】这句简单的特征提取降采的逻辑并不是使用torchaudio实现，而是在代码实现发现他居然是使用卷积实现的。这就让我陷入自我怀疑。结果对于代码的分析和知识的查询发现

他的方法是使用固定的fram_rate[fram_rate: 帧率,单位时间内使用的窗口数量]来处理音频。因此可以通过以下公式计算每个窗口大小。然后将每个窗口特征降采样为一个dim=512 的特征。

`$$ fram\_size= \frac {sameple\_rate}{fram\_rate} $$`


![未命名绘图](未命名绘图.jpg)

如图所示一个语音采样率为8【单位时间采样率8个点为】.帧率为4【单位时间的数据分为4个窗口】帧为2 【因此每个窗口的大小为2。】 

我们每次处理的音频为一个帧，接下来要做的事情就是将他降采样后使用一个dim为512的特征来表示他。但是这个过程是如何实现的呢？就要引出我们的CNN的重新采样。



##  CNN 中的下采样和上采样

毫无疑问，卷积神经网络给计算机视觉领域带来了巨大的进步，在这篇文章中，我将与您一起简短地了解它的一些概念，特别是 CNN 中的下采样和上采样。是的，换句话说卷积的过程本身就是在重采样。特别的语音信号本身也是一个频谱图。

先通过一张图直观的来描述cnn卷积和逆卷积的过程

![overall](overall.png)

### 1、回顾CNN的概念：

卷积神经网络是一种尝试使用过滤器从图像中提取特征，然后将这些特征映射到类或标签的技术。以下的图片是描述它是如何工作的

![cnn](cnn.png)

大小为 (f,f) 的滤波器将会从输入的左上角开始，每次对输入和滤波器大小相同块进行元素乘法，然后求和，并将总和作为输出的第一个像素。然后通过称为步幅 (S) 的移动步骤移动到下一个图像块。以此类推最终得倒卷积后的特征。我们还可以使用所谓的填充（p）来将输出形状保留为输入形状。通过这种绝妙的方式，我们可以从这样的图像中提取垂直或水平线甚至圆形等特征。

![2022_12_12_13e_Kleki](2022_12_12_13e_Kleki.png)



### 2、在CNN中的下采样：

但是从第一层提取特征之后呢？！我们应该将这个输出直接输入到第二个卷积层吗？！事实上，这在计算上会很昂贵，因此我们更愿意减少输出的大小，同时对提取的特征影响最小，这就是所谓的***下采样***。而这个操作是最全卷积网络的前半部分。输出的尺寸将遵循以下等式：

`$$ Output\_Size = (\frac { n + 2p -f }{S} +1) \;\times \;  (\frac { n + 2p -f }{S} +1) $$`


其中：

n = 输入高度或宽度

p = 填充

s = 步幅

f = 滤波器尺寸

因此，很明显，增加分母（步幅）将导致输出大小缩小。



### 3、下采样的不同方法：

除了使用步幅 > 1 进行卷积以减少输出大小以进行下采样之外，还有另一种非常著名的方法，称为池化（在大多数情况下使用最大池化而不是平均池化），通过这种方式，我们定义了一个pool_size，在池化窗口（大小由 pool_size 定义）上取最大值（或取平均值），也带有移动步长（步幅）。因此，poo_size 为 (2,2) 且步长为 2 的 maxpooling 将使输出为输入大小的 25%。

![download](download.png)

这些方法通过减小尺寸而在计算上非常有效，而不会造成特征的严重影响。（在狗特征的图像中非常清晰）。

在pytorch中两种方法实现都比较简单

```python
# 定义一个卷积层，输入通道数为3，输出通道数为16，卷积核大小为3x3
conv_layer = nn.Conv2d(in_channels = 3, out_channels = 16, kernel_size = 3)
```

```python
# 定义最大池化层，这里以二维最大池化为例，池化核大小为2x2，步长为2
max_pool = nn.MaxPool2d(kernel_size=2, stride=2)
```



### 4、CNN中的上采样

但是，如果我们希望输出与输入大小相同，“想象一下，您处于语义分割任务中，输入的每个像素都将分配给一个标签，因此您需要与输入大小相同的输出”。这意味着我们需要反转“下采样”。从小尺寸图像开始，我们需要达到之前的尺寸。*“实际上我们只需要达到相同的大小而不是精确的特征图”* ，这就是所谓的***上采样***这是全卷积网络的后半部分，寻求返回到原始大小。有很多方法可以做到这一点。

![Bilinear@2x](Bilinear@2x.jpg)

### 5、上采样的不同方式：

- ***简单上采样（最近邻插值）*** ：

从它的名字来看，这是一个非常简单且计算成本低的操作，它只需根据上采样因子复制或重复行，然后重复列。

![upsampling1](upsampling1.png)

这里的上采样因子是 (2,2)，因此将行加倍，然后将列加倍，从而增加输出大小。

在 pytorch 中轻松实现：

```python
import torch
import torch.nn.functional as F


def upsampling_2d_nearest(input_tensor):
    """
   （最近邻插值，上采样倍数为2）
    """
    # 设置上采样的尺寸，这里对应2倍上采样
    scale_factor = 2
    # 使用torch.nn.functional.interpolate进行上采样，采用最近邻插值
    output_tensor = F.interpolate(
        input_tensor, scale_factor=scale_factor, mode='nearest'
    )
    return output_tensor

x = torch.tensor(
    [
        [1,2],
        [3,4]
    ],
    dtype=float
)
# 创建一个示例的输入张量（形状为batch_size=1, channels=1, height=2, width=2）
input_tensor = x[None][None]
# 进行上采样
output_tensor = upsampling_2d_nearest(input_tensor)

output_tensor
# tensor([[[[1., 1., 2., 2.],
#           [1., 1., 2., 2.],
#           [3., 3., 4., 4.],
#           [3., 3., 4., 4.]]]], dtype=torch.float64)
```

在 keras 中轻松实现

```python
tf.keras.layers.UpSampling2D(
    size=(2, 2), data_format=None, interpolation="nearest", **kwargs
)
```



- ***取消池化***：

我们可以不简单地重复像素，而是反转下采样中使用的操作，使用**Un-pooling**反转池化，如下所示：

![Example-of-the-process-of-max-pooling-and-upsampling-layers-The-pooling-layer](Example-of-the-process-of-max-pooling-and-upsampling-layers-The-pooling-layer.png)

不会返回到相同的精确像素，但至少返回到与最重要的像素相同的分辨率。

- ***转置卷积***：

或者我们可以使用所谓的转置卷积或反卷积（“这在数学上是不正确的术语”）或 unconv 或部分跨步卷积来反转卷积层，这是最常见的上采样方式，我们只需将常规卷积中发生的情况转置如下。

![kXkDB](kXkDB.png)

输入的每个像素与内核相乘，并将输出（与内核大小相同）放入最终输出特征图中，并再次以步幅移动（但这次步幅是在最终的输出），所以这里增加步幅将增加输出大小，另一方面增加填充会减少输出大小。以下是输出大小的公式：

`$$ output\;size\;=\; (input\_size-1)\times stride - 2\times padding + (kernel\_size -1)+1 $$`


正如我们所看到的，步幅和填充对输出大小具有相反的影响，而不是它们在正常卷积中的影响。

这里是如何在 pytorch 中使用的：

```python
tconv = nn.ConvTranspose2d(1, 1, kernel_size=2, padding=1, bias=False)
```



***SUMMARY 概括***

| Operation           | 对输出的影响 | 类型                                        |
| :------------------ | :----------- | :------------------------------------------ |
| Downsampling 下采样 | 减小尺寸     | 标准卷积，步长 >1<br />池化（最大或平均值） |
| Upsampling 上采样   | 增加尺寸     | 最近插邻、反池化和转置卷积                  |



## Moshi中的实现

首先回到目标 Mimi的目标是的要将语音信号转换为一个token。在这个过程中需要先将24KHz的采样率音频分帧处理。帧率为12.5Hz。然后将每个帧映射为一个dim = 512 维度的大小。然而映射到512这个降采样的操作就是今天的重头戏。每个帧的特征如何得来的，如果你忘记了可以重新看看背景部分。

接下来开始先看具体代码：

> 忽律了与本次核心讲解无关的代码

在moshi_fast_pt/moshi_fast_pt/models/loaders.py定义了 mimi模型加载的逻辑

### 0、模型加载

```python
def get_mimi(filename: str | Path,
             device: torch.device | str = 'cpu') -> MimiModel:
    """Return a pretrained Mimi model."""
    encoder = SEANetEncoder(**_seanet_kwargs)
    encoder_transformer = transformer.ProjectedTransformer(
        device=device, **_transformer_kwargs
    )
    model = MimiModel(
        encoder,
        sample_rate=SAMPLE_RATE,
        frame_rate=FRAME_RATE,
        encoder_frame_rate=SAMPLE_RATE / encoder.hop_length,
        encoder_transformer=encoder_transformer,
    ).to(device=device)
    return model
  
```



### 1、StreamingConv1d

```python
class StreamingConv1d(StreamingModule[_StreamingConv1dState]):
    """Conv1d with some builtin handling of asymmetric or causal padding
    and normalization.
    """

    def __init__(
        self,
    ):
      pass
    @property
    def _padding_total(self) -> int:
        return self._effective_kernel_size - self._stride
    
    @property
    def _effective_kernel_size(self) -> int:
        dilation = self.conv.conv.dilation[0]
        return (
            self._kernel_size - 1
        ) * dilation + 1  # effective kernel size with dilations

    def forward(self, x):
       
        padding_total = self._padding_total
 
        # 对输入进行填充，防止在卷积的过程有 最后一个时间步无用而导致卷积不完整
        extra_padding = get_extra_padding_for_conv1d(
            x, self._effective_kernel_size, self._stride, padding_total
        )
        
        x = pad1d(x, (state.padding_to_add, 0), mode=self.pad_mode)
        y = self.conv(x)
        return y
```



1、首先将我们的注意力集中到他的一维流式卷积StreamingConv1d。这里请先把他当作一个普通的卷积，因为我们讨论的主题与流式无关。

上面说到卷积输出的大小是可以计算的，公式如下：

`$$ O =\frac { N + 2p -K }{S} +1 $$`

- 可以不看的部分：自定义填充这部分是谁为了处理特殊情况。为了简单我们只考虑常规情况也就是一个完整的帧

不同的是，这小子把填充的逻辑写了，目的对卷积进行填充以确保最后一个窗口是完整的。在末尾添加额外的填充。这是为了确保我们可以重建相同长度的输出，因为否则，即使有填充，某些时间步长也可能会被删除

```python
def get_extra_padding_for_conv1d(
    x: torch.Tensor, kernel_size: int, stride: int, padding_total: int = 0
) -> int:
    # 对输入进行填充，防止在卷积的过程有 最后一个时间步无用而导致卷积不完整
    """See `pad_for_conv1d`."""
    length = x.shape[-1]
    n_frames = (length - kernel_size + padding_total) / stride + 1
    ideal_length = (math.ceil(n_frames) - 1) * stride + (kernel_size - padding_total)
    return ideal_length - length


def pad_for_conv1d(
    x: torch.Tensor, kernel_size: int, stride: int, padding_total: int = 0
):
    """对卷积进行填充以确保最后一个窗口是完整的。在末尾添加额外的填充。这是为了确保我们可以重建相同长度的输出，因为否则，即使有填充，某些时间步长也可能会被删除。
    For instance, with total padding = 4, kernel size = 4, stride = 2:
        0 0 1 2 3 4 5 0 0   # (0s are padding)
        1   2   3           # (output frames of a convolution, last 0 is never used)
        0 0 1 2 3 4 5 0     # (output of tr. conv., but pos. 5 is going to get removed as padding) 转置卷积的输出结果（output of tr. conv.），转置卷积可以看作是卷积操作的一种 “逆” 过程（但不完全是严格数学意义上的逆），它会根据之前卷积的相关参数以及自身的特性来生成输出
            1 2 3 4         # once you removed padding, we are missing one time step !
    """
    extra_padding = get_extra_padding_for_conv1d(x, kernel_size, stride, padding_total)
    return F.pad(x, (0, extra_padding))

```

- 可以不看的部分结束

这里我们看到在卷积之前它先对输入进行了一次填充。填充大小为

```python
_padding_total = _effective_kernel_size - _stride
_effective_kernel_size =  (
           _kernel_size - 1
        ) * dilation + 1  # effective kernel size with dilations
其中 dilation始终等于1 因此可以换算出填充大小
_padding_total = _kernel_size - _stride

** 因此输入的大小就变成 N + (_kernel_size - _stride)
** 由于填充是在卷积之前做的 所以 P =0

将这些结果带入原公式：

O = ((N + k - s) + 2 *0  -k) / s + 1
  = (N-s)/s + 1
  = N / s

```

更新一下每个卷积输出的大小公式为

`$$ O = \frac {N}{S} $$`



### 2、SEANetEncoder

```python
class SEANetEncoder(StreamingContainer):
    def __init__(
        self,
      	ratios,
    ):
      self.ratios = list(reversed(ratios))
      self.n_residual_layers = 1
      self.residual_kernel_size = 3
      self.hop_length = int(np.prod(self.ratios))
      model: tp.List[nn.Module] = [
            StreamingConv1d(
                stride = 1
            )
        ]
      
      for i, ratio in enumerate(self.ratios):
        for j in range(n_residual_layers):
                model += [
                    SEANetResnetBlock(
                      	stride = 1
                    )
                ]

            # Add downsampling layers
            model += [
                act(**activation_params),
                StreamingConv1d(
                    stride=ratio,
                ),
            ]
     	model += [
            act(**activation_params),
            StreamingConv1d(
                stride = 1
            ),
        ]

        # self.model = nn.Sequential(*model)
      
      
      
```

再看SEANetEncoder，这个时候其实我们已经知道输出的大小只和stride有关。我们会发现。所有的降采样的操作都是发生在 # Add downsampling layers 对应的代码块部分

也就是说最终模型输出的带下为

```python
for s in ratios:
	N = N / S

也就是

N / int(np.prod(self.ratios))

```



现在我们的模型的输出结构已经变成了 1，512，N / int(np.prod(self.ratios)) 接下来的是就是讲 N / int(np.prod(self.ratios))变为 1 聪明的你会发现  当stride = x 的时候 代入N/S 不就刚好为1吗。



它也的确是这样做的：



### 3、MimiModel的结构

```python
class MimiModel(CompressionModel[_MimiState]):
   def __init__(
        self,
   ):
      self.encoder_frame_rate = SAMPLE_RATE / encoder.hop_length
      self.frame_rate=FRAME_RATE
      downsample_stride = self.encoder_frame_rate / self.frame_rate
      self.downsample = ConvDownsample1d(
            int(downsample_stride),
            dimension=dimension,
            learnt=learnt,
            causal=causal,
      )
  def encode(self, x: torch.Tensor) -> torch.Tensor:
        emb = self._encode_to_unquantized_latent(x)
        codes = self.quantizer.encode(emb)
        return codes
  def _encode_to_unquantized_latent(self, x: torch.Tensor) -> torch.Tensor:
        emb = self.encoder(x)
        emb = self._to_framerate(emb)
        return emb
  def _to_framerate(self, x: torch.Tensor):
        return self.downsample(x)
  
```

这儿的ConvDownsample1d的底层首先也是流式卷积 所以对应的公式还是满足

来代入一下吧

``` python
最开始输入模型的N
  N = SAMPLE_RATE / frame_rate
经过Encoder 后的 N
	N = （SAMPLE_RATE / frame_rate）/ int(np.prod(self.ratios))
经过downsample 后的N
  N = (（SAMPLE_RATE / frame_rate）/ int(np.prod(self.ratios)) ) / ((SAMPLE_RATE / encoder.hop_length) /  FRAME_RATE)
  = 1
	
```



总结：

StreamingConv1d 填充的实现方式保证了模型输出的现状只和 S 有关也就是

`$$ O = \frac {N}{S} $$`


因此经过SEANetEncoder的过程里面包含 卷句的S 列表为 ratios = [8, 6, 5, 4]，所以模型经过SEANetEncoder 后的输出的形状为

`$$ O_{encoder} = \frac {O}{np.prod(ratios)} $$`


最后MimiModel downsample保证了模型的输出为1

`$$  O = \frac {（SAMPLE\_RATE / frame\_rate）/ int(np.prod(self.ratios)) {((SAMPLE\_RATE / encoder.hop\_length) /  FRAME\_RATE)} \\ O = 1 $$`


参考文档：

https://github.com/kyutai-labs/moshi

https://d2l.ai/chapter_computer-vision/transposed-conv.html

https://iq.opengenus.org/downsampling-and-upsampling-in-cnn/

## 
