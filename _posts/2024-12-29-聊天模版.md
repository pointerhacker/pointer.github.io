---
layout: post
title: 大模型对话模版
categories: [大模型]
tags: 机器学习
---
## 是什么
> 用自然语言的方式结构化化描述用户的机器交互的过程

### Base Model和Instruct Model
基本模型根据原始文本数据进行训练以预测下一个标记，而指令模型则专门进行微调以遵循指令并参与对话。
例如， Qwen/Qwen2.5-0.5B 是基本模型，而Qwen/Qwen2.5-0.5B-Instruct是其指令微调模型。
为了使基础模型表现出指令遵循的能力，我们需要以模型可以理解的一致方式格式化提示。这就是聊天模板的用武之地。
ChatML 就是这样一种模板格式，它可以通过明确的角色指示符（系统、用户、助理）来构建对话。
需要注意的是，Base Model可以在不同的聊天模板上进行微调，因此当我们使用Instruct Model时，我们需要确保使用正确的聊天模板。

- ChatML
> OpenAI Introduced Chat Markup Language (ChatML) Based Input To Non-Chat Modes

OpenAI 为Base模型引入了基于聊天标记语言 (ChatML) 的输入。目的是希望所有用户从文本补全迁移到聊天补全

### 了解对话模版
聊天模板的核心是定义了与语言模型交互时的对话结构。
这个结构包括了`system-level instructions【系统级指令】, user messages【用户消息】, and assistant response【助手响应】` 这些主要信息，并将他按自然语言的形式描述，使模型可以理解
这种结构有助于保持交互之间的一致性，并确保模型对不同类型的输入做出适当的响应。
以下是聊天模板的示例：
```xml
<|im_start|>user
Hi there!<|im_end|>
<|im_start|>assistant
Nice to meet you!<|im_end|>
<|im_start|>user
Can I ask a question?<|im_end|>
<|im_start|>assistant
```
transformers库将使用对应模型的tokenizer自动为您处理聊天模板
[在此处](https://huggingface.co/docs/transformers/en/chat_templating#how-do-i-use-chat-templates)阅读有关 Transformers 如何构建聊天模板的更多信息。

我们所要做的就是以正确的方式构建我们的消息，标记器将处理其余的事情。这是对话的基本示例：
```python
messages = [
    {"role": "system", "content": "你是一个非常有用的聊天助手"},
    {"role": "user", "content": "你可以接受一下chat template是什么东西吗?"},
    {"role": "assistant", "content": "聊天模板构建了用户与人工智能模型之间的对话结构..."}
]
```
让我们深入上面的示例，看看它如何映射到聊天模板格式。

### 系统指令 [System Messages]
系统指令为模型的行为、能力、角色等信息进行了定义。它们充当影响所有后续交互的持久指令。
例如：
```python
system_message = {
    "role": "system",
    "content": "您是一名专业的客户服务代理。始终保持礼貌、清晰且乐于助人。"
}
```
### 对话
聊天模板通过对话历史记录维护上下文，存储用户和助理之间以前的交流。这允许更连贯的多轮对话：
```python
conversation = [
    {"role": "user", "content": "我的订单需要帮助"},
    {"role": "assistant", "content": "我很乐意提供帮助。您能提供您的订单号吗？"},
    {"role": "user", "content": "12138"},
]
```
### Transformer 相关实现
Transformers 库提供了对聊天模板的内置支持。
以下是如何使用它们：
```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("HuggingFaceTB/SmolLM2-135M-Instruct")

messages = [
    {"role": "system", "content": "You are a helpful coding assistant."},
    {"role": "user", "content": "Write a Python function to sort a list"},
]

# Apply the chat template
formatted_chat = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
```

#### 代码细节
[源代码](https://github.com/huggingface/transformers/blob/main/src/transformers/tokenization_utils_base.py)

```python
{
    "chat_template": "{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\nYou are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}"
}
```

```python
def apply_chat_template(self):
    chat_template = self.get_chat_template(chat_template, tools)
    compiled_template = _compile_jinja_template(chat_template)
    ### 并行推理的支持
    for chat in conversations:
        rendered_chat = compiled_template.render(
            messages=chat,
        )
    rendered.append(rendered_chat)
```

可以发现其实就是使用Jinja2 模板代码片段 循环处理json最终成为了外面期望的格式

### 自定义对话模版格式
您可以自定义不同消息类型的格式。
例如，为不同的角色添加特殊标记或格式：
```python
template = """
<|system|>{system_message}
<|user|>{user_message}
<|assistant|>{assistant_message}
""".lstrip()
```


### 多轮对话支持
```python
messages = [
    {"role": "system", "content": "You are a math tutor."},
    {"role": "user", "content": "What is calculus?"},
    {"role": "assistant", "content": "Calculus is a branch of mathematics..."},
    {"role": "user", "content": "Can you give me an example?"},
]
```

